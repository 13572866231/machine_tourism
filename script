cat("\014")

#Set seed
seed <-777

# Required packages
library(corrplot)
library(dplyr)
library(caret)
library(dygraphs)
library(wordcloud)
library(RColorBrewer)
library(tempdisagg)
library(xts)
library(randomForest)
library(RSNNS)
library(pls)
library(forecast)
library(CombMSC)
library(neuralnet)
 
reciepts <- read.csv("quarterly_receipts.csv")# Read in data
googletrends <- read.csv("google_trends.csv")
wordcloud <- read.csv("wordcloud.csv")
arrivals_nights <- read.csv("arrivals_nights.csv")
aux <- read.csv("auxiliary.csv")

# convert to time series
quarterly_reciepts <- ts(reciepts$Tourism.receipts, start=c(2005, 1), end=c(2016, 4), frequency=4) 
tot_arrivals <- ts(arrivals_nights$Arrivals, start=c(2004,1), end=c(2016,12), frequency=12)
tot_nights <- ts(arrivals_nights$Nights, start=c(2004,1), end=c(2016,12), frequency=12)
time <- ts(arrivals_nights$time, start=c(2004,1), end=c(2016,12), frequency=12)
d1 <- ts(arrivals_nights$d1, start=c(2004,1), end=c(2016,12), frequency=12)
d2 <- ts(arrivals_nights$d2, start=c(2004,1), end=c(2016,12), frequency=12)
d3 <- ts(arrivals_nights$d3, start=c(2004,1), end=c(2016,12), frequency=12)
A_cycle <- ts(aux$A_cycle, start=c(2004,1), end=c(2017,2), frequency=12)
M_cycle <- ts(aux$M_cycle, start=c(2004,1), end=c(2017,2), frequency=12)

# Convert to date
googletrends$Month <- as.Date(as.factor(googletrends$Month), format("%Y%m%d"))
# Convert to xts
x <- xts(googletrends, order.by = googletrends$Month) 

#Wordcloud
pal <- brewer.pal(9, "Dark2")
wordcloud(words = wordcloud$Word, freq = wordcloud$Freq, min.freq = 1,
          , random.order=FALSE, rot.per=0.35, 
          colors=pal)

#Subset
tour.us.data  <- select(googletrends,contains("United.States"))
tour.ven.data <- select(googletrends,contains("Venezuela"))
tour.col.data <- select(googletrends,contains("Colombia"))
tour.ned.data <- select(googletrends,contains("Netherlands"))
tour.can.data <- select(googletrends,contains("Canada"))

# calculate and plot correlation matrix
corMat_tour.us.data <- cor(tour.us.data[, -1])
corMat_tour.ven.data <- cor(tour.ven.data[, -1])
corMat_tour.col.data <- cor(tour.col.data[, -1])
corMat_tour.ned.data <- cor(tour.ned.data[, -1])
corMat_tour.can.data <- cor(tour.can.data[, -1])

corrplot.mixed(corMat_tour.us.data,  order = "hclust", upper ="pie")
corrplot.mixed(corMat_tour.ven.data, order = "hclust", upper ="pie")
corrplot.mixed(corMat_tour.col.data, order = "hclust", upper ="pie")
corrplot.mixed(corMat_tour.ned.data, order = "hclust", upper ="pie")
corrplot.mixed(corMat_tour.can.data, order = "hclust", upper ="pie")

# Chow-Lin disaggregation
td <- td(quarterly_reciepts ~  tot_arrivals + tot_nights + time + d1 + d3,
         conversion = "sum", to = "monthly", method = "chow-lin-minrss-ecotrim")
monthly_reciepts <- predict(td)
summary(td)
plot(td)

graph_decomposition_fit <- dygraph(cbind(td$actual, td$fitted.values, main="Chow-Lin disaggregation fit"))%>%
        dyAxis("y", label = "In Afl. million")

graph_monthly_reciepts <- dygraph(monthly_reciepts, main="Chow-Lin disaggregation")%>%
        dyOptions(fillGraph = TRUE, fillAlpha = 0.4) %>%
        dyAxis("y", label = "In Afl. million") %>%
        dyOptions( fillGraph = TRUE, drawGrid = FALSE) 

graph_decomposition_fit
graph_monthly_reciepts

monthly_reciepts_components <- decompose(monthly_reciepts)
plot(monthly_reciepts_components)

# Principle component 
pca<-prcomp(googletrends[, -1],center = TRUE, 
     scale. = TRUE)  
print(pca) 
plot(pca, type="1") 
summary(pca) 
screeplot(pca, type="lines") 
pca1<-pca$x[,1] 
pca2 <- pca$x [,2] 
pca3 <- pca$x [,3] 
pca4 <- pca$x [,4] 
pca5 <- pca$x [,5] 

pca1 <- ts(pca1, start=c(2004, 1), end=c(2017, 2), frequency=12) 
pca2 <- ts(pca2, start=c(2004, 1), end=c(2017, 2), frequency=12)  
pca3 <- ts(pca3, start=c(2004, 1), end=c(2017, 2), frequency=12)  
pca4 <- ts(pca4, start=c(2004, 1), end=c(2017, 2), frequency=12)  
pca5 <- ts(pca4, start=c(2004, 1), end=c(2017, 2), frequency=12)

#Normalization
pca1 = (pca1-min(pca1))/(max(pca1)-min(pca1))
pca2 = (pca2-min(pca2))/(max(pca2)-min(pca2))
pca3 = (pca3-min(pca3))/(max(pca3)-min(pca3))
pca4 = (pca4-min(pca4))/(max(pca4)-min(pca4))
pca5 = (pca5-min(pca5))/(max(pca5)-min(pca5))
A_cycle = (A_cycle-min(A_cycle))/(max(A_cycle)-min(A_cycle))
M_cycle = (M_cycle-min(M_cycle))/(max(M_cycle)-min(M_cycle))
monthly_reciepts_n = (monthly_reciepts-min(monthly_reciepts))/
        (max(monthly_reciepts)-min(monthly_reciepts))

# Split test and training sets
reciepts.split <- splitTrainTest(monthly_reciepts_n,numTrain = length(monthly_reciepts_n) - 24)
pca1 <- splitTrainTest(pca1,numTrain = length(monthly_reciepts_n) - 24)
pca2 <- splitTrainTest(pca2,numTrain = length(monthly_reciepts_n) - 24)
pca3 <- splitTrainTest(pca3,numTrain = length(monthly_reciepts_n) - 24)
pca4 <- splitTrainTest(pca4,numTrain = length(monthly_reciepts_n) - 24)
pca5 <- splitTrainTest(pca5,numTrain = length(monthly_reciepts_n) - 24)
time <- splitTrainTest(time,numTrain = length(monthly_reciepts_n) - 24)
A_cycle <- splitTrainTest(A_cycle,numTrain = length(monthly_reciepts_n) - 24)
M_cycle <- splitTrainTest(M_cycle,numTrain = length(monthly_reciepts_n) - 24)

pca.train <- cbind(reciepts.split$train,pca1$train,pca2$train,pca3$train,pca4$train,pca5$train, A_cycle$train, M_cycle$train)
pca.test <- cbind(reciepts.split$test,pca1$test,pca2$test,pca3$test,pca4$test,pca5$test, A_cycle$test, M_cycle$test)
pca.test <-as.xts(pca.test)
pca.train <- as.xts(pca.train)
colnames(pca.test) <- c("reciepts","pca1","pca2","pca3","pca4","pca5","A_cycle", "M_cycle")
colnames(pca.train) <- c("reciepts","pca1","pca2","pca3","pca4","pca5","A_cycle", "M_cycle")


# Random forest ####Not working? Graphing?
set.seed(seed)
rf.fit <- randomForest(reciepts ~ ., 
                   data = pca.train,importance=TRUE,
                         proximity=TRUE,
                   na.action=na.omit,
                   ntree=2000)
print(rf.fit)
plot(rf.fit, main='')
varImpPlot(rf.fit, main='')

rf.predict <- predict(rf.fit, pca.test)
print(rf.predict)

graph_rf <- dygraph(cbind(reciepts.split$train,
                          rf.fit.predicted_t,
                          rf.predict_t,
                          reciepts.split$test))%>%
        dyShading(from = "2004-1-1", to = "2014-12-12", color = "#FFE6E6") %>%
        dyShading(from = "2015-1-1", to = "2016-12-12", color = "#CCEBD6")
graph_rf

rf.predict_t <- ts(rf.predict, start=c(2015,1), end=c(2017,2), frequency=12)
rf.fit.predicted_t <- ts(rf.fit$predicted, start=c(2004,1), end=c(2014,12), frequency=12)








#Nueral net
set.seed(seed)

#Neural network autoregression
reciepts.nnetar <- nnetar(reciepts.split$train, repeats = 20,p=12,P=1,size=7)
summary(reciepts.nnetar$model[[1]])
reciepts.nnetar.pred <- forecast(reciepts.split$train, h=36)
accuracy(reciepts.nnetar.pred, reciepts.split$test)
plot(reciepts.nnetar.pred)

plot(reciepts.split$train)
lines(reciepts.nnetar.pred$fitted, 
      lwd = 2, col = "blue") 
lines(reciepts.nnetar.pred$mean, 
      lwd = 2, col = "blue", lty = 2) 
lines(reciepts.split$test)

graph_nnetar <- dygraph(cbind(reciepts.split$train,
                              reciepts.nnetar.pred$fitted,
                              reciepts.nnetar.pred$mean,
                              reciepts.split$test))%>%
        dyShading(from = "2004-1-1", to = "2014-12-12", color = "#FFE6E6") %>%
        dyShading(from = "2015-1-1", to = "2016-12-12", color = "#CCEBD6")
graph_nnetar

# avNNet()
reciepts.avnnet<- avNNet(reciepts ~ ., data=pca.train, repeats=25, size=3, decay=0.1,
               linout=TRUE)
summary(reciepts.avnnet$model[[1]])
reciepts.avnnet.pred <- forecast(pca.train$reciepts, h=36)



##info: No, but it isn't hard to do it using the avNNet function from the caret package. 
# Basically nnetar() is forecasting using an average of 20 neural nets where the inputs 
#are lagged values of the time series. You would just need to add the xreg variables as 
#inputs as well.

#To implement avNNet() on a time series dataset, would you create a dataframe that holds 
#not only dummy variables for holiday/seasonal effects but also the lagged variables? 
#Eg something like avNNet(point_forecast ~ lagged_1 + lagged_2 + lagged_3 + holiday_1) 
#where lagged_1,..,lagged_3 are the lagged variables for that daily forecast? 
# (And thus not use nnetar() at all?)


# Neural net
nn <- neuralnet(reciepts ~ pca1 + pca2 + pca3 + pca4 + pca5 + A_cycle + M_cycle,
                data=pca.train,hidden=c(5,3),linear.output=T)
plot(nn)

pr.nn <- compute(nn,pca.test[,1:7])

w.pr.nn <- pr.nn$net.result*(max(monthly_reciepts)-min(monthly_reciepts))+min(monthly_reciepts)
w.pr.nn

w.reciepts_test <- (pca.test$reciepts)*(max(monthly_reciepts)-min(monthly_reciepts))+min(monthly_reciepts)
w.reciepts_test


joint <- dygraph(cbind(w.pr.nn, w.reciepts_test))
joint

#subject to revision
MSE.nn <- sum((w.reciepts_test - w.pr.nn)^2)/nrow(w.reciepts_test)
